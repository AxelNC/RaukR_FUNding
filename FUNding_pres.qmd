---
title: "FUNding"
author: "Muricio Roca, Axel Cedholm, Joel Hallgren, Jose A. L. V., Evelyn Halitzki, Mikael Brink"
format: revealjs
menu:
  slide-number: true
  show-slide-number: print
editor: visual
---

## Background

The Swecris database enables you to search, compare and produce statistics for Swedish research projects.

-   Information from several research funding bodies is gathered in one place.

    <https://www.vr.se/english/swecris.html#/>

## Retrieval of data

We used the swecris library available at github, <https://github.com/KTH-Library/swecris> to download the data.

```{r}
#| label: load-packages
#| echo: true
#| eval: false
 
#install
install_github("KTH-Library/swecris", dependencies = T, force=T)
#
library(swecris)
allfunding<-swecris::swecris_fundings()

#I modified the following function from swecris, and removed the search string that specified KTH
#kth_projects <- swecris_funding()

swecris_all_university_projects <- function (searchstring = "", 
                                             token) 
{
  if (missing(token)) 
    token <- "RWNDZ3FDRVVSMmNUNlZkMkN3"
  httr::GET("https://swecris-api.vr.se/v1/scp/export", query = list(`organizationType[]` = "Universitet", 
                                                                    sortOrder = "desc", sortColumn = "FundingStartDate", 
                                                                    searchText = URLencode(searchstring), token = token)) %>% 
    httr::content(as = "text", encoding = "UTF-8") %>% readr::read_delim(delim = ";", 
                                                                         quote = "\"", show_col_types = FALSE)
}
#get the data to an object
all_university_projects <- swecris_all_university_projects()

```

## Background {.smaller}

```{r, echo=F}
library(tidyverse)
library(gt)
all_university_projects <- readRDS("all_university_projects.RDS")
```

The downloaded data contains these headers, and contains

-   There are **`r nrow(all_university_projects)`** projects in our data.
-   **`r all_university_projects %>% distinct(FundingOrganisationNameEn) %>% count()`** unique funding organizations
-   **`r all_university_projects %>% distinct(CoordinatingOrganisationNameEn) %>% count()`** unique coordinating organizations
-   The database also contained one abstract for each project.

```{r}

all_university_projects %>% colnames()
```

## Aim

-   Rshiny- app to display and interactively change

    -   **Descriptive data**

    -   A **map** displaying the geographical distribution of funds in Sweden for Universities.

    -   **Word cloud** displaying the most prevalent words for the respective research field

    -   Information related to the **Grant applicants**

## Wordcloud process {.smaller}

::: columns
::: {.column width="60%"}
```{r word process}
#| label: word processing
#| echo: true
#| eval: false
#add filtered column for the second column
scbs.codes.mini <- scbs.codes %>% filter(scb_code <10)
#nest by people
proj_nest1<- nest_join(x = all_university_projects, y = involved.people, by = "ProjectId")
#add nest by scbcodes (reduced to codes <10)
proj_nest2 <- nest_join(x = proj_nest1, y = scbs.codes.mini, by = "ProjectId")
#view data
proj_nest2 %>% glimpse()
#make a tibble with fewer cols
proj_nest2.small<- proj_nest2 %>% select(ProjectId,
                                         ProjectAbstractEn,
                                         CoordinatingOrganisationNameEn, 
                                         FundingsSek,
                                         FundingYear,
                                         CoordinatingOrganisationTypeOfOrganisationEn,
                                         FundingOrganisationTypeOfOrganisationEn,
                                         involved.people, 
                                         scbs.codes.mini)


#make a row of each word for each abstract
tokenized <- proj_nest2.small %>% unnest_tokens(word, 
                                                input = ProjectAbstractEn, 
                                                to_lower = T)
#delete so called stop-words
token_wo_stopwords <- tokenized %>% anti_join(stop_words)

library(wordcloud)
library(tm)
#make wordcloud from data
# category 1
token_wo_stopwords %>% 
  count(word, sort = TRUE, name = "nr_words") %>%  
  slice_max(order_by = nr_words, n = 300) %>% {wordcloud(.$word, .$nr_words)}
```
:::

::: {.column width="40%"}
All abstracts ( **`r all_university_projects %>% filter(!is.na(ProjectAbstractEn)) %>% count()`** ) were:

-   Tokenized using `tidytext()` package & all words set to lowercase.

-   Stopwords (e.g. if, or, but, also) removed by `anti_join(stop_words)`

-   Word counted by `base::count`

-   Wordcloud produced using `wordcloud` package
:::
:::

## Map

the library(leaflet) was used to plot the map.

## Challenges

-   Rshiny implementation
-   

## Thank you RaukR!
